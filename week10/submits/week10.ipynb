{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Download and inspect the data\n",
    "READS folder, containing paired-end read data from 8 samples. For simplicity, you can consider these 8 samples to be gut samples collected from the infant from day 0 to day 8.\n",
    "\n",
    "assembly.fasta file, containing the joint assembly of all the samples. Note that entire genomes of the gut bacteria cannot be assembled. Instead, these assembled sequences are the broken up and scrambled pieces of all the various bacterial genomes.\n",
    "\n",
    "KRAKEN folder, containing the approximate classifications of both the reads and the assembled scaffolds. KRAKEN is a very fast and powerful tool that uses k-mer profiles instead of direct alignment to match sequences to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Investigate the taxonomic profile of the reads\n",
    "view a detailed taxonomic breakdown of all 8 samples to see how the gut microbiota changes across the days following birth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create --name week10 krona metabat2 jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!/bin/bash\n",
    "#for SAMPLE in 83 86 88 89 90 93 94 97\n",
    "#do\n",
    "#    cut -c 11- SRR4921${SAMPLE}.kraken | awk '{gsub(\";\",\"\\t\"); print}' > day_${SAMPLE}.kraken\n",
    "#done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ktImportText day_83.kraken\n",
    "#mv text.krona.html ../../../submits/day_83.html\n",
    "\n",
    "#ktImportText day_86.kraken\n",
    "#mv text.krona.html ../../../submits/day_86.html\n",
    "\n",
    "#ktImportText day_88.kraken\n",
    "#mv text.krona.html ../../../submits/day_88.html\n",
    "\n",
    "#ktImportText day_89.kraken\n",
    "#mv text.krona.html ../../../submits/day_89.html\n",
    "\n",
    "#ktImportText day_90.kraken\n",
    "#mv text.krona.html ../../../submits/day_90.html\n",
    "\n",
    "#ktImportText day_93.kraken\n",
    "#mv text.krona.html ../../../submits/day_93.html\n",
    "\n",
    "#ktImportText day_94.kraken\n",
    "#mv text.krona.html ../../../submits/day_94.html\n",
    "\n",
    "#ktImportText day_97.kraken\n",
    "#mv text.krona.html ../../../submits/day_97.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ktImportText day_88.kraken > day_88.html\n",
    "ktImportText day_89.kraken > day_89.html\n",
    "ktImportText day_90.kraken > day_90.html\n",
    "ktImportText day_93.kraken > day_93.html\n",
    "ktImportText day_94.kraken > day_94.html\n",
    "ktImportText day_97.kraken > day_97.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Briefly comment on the trends you see in the gut microbiota throughout the first week.\n",
    "\n",
    "While enterobacter remains the dominant species across the week, we see an initial high diversity on day one, followed by a steep drop for day 2, and then a steady gain of diversity through the rest of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Deconvolute the assembled scaffolds into individual genomes (binning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at pie charts is a good way to get a feel for what the microbial communities look like, but we want to know more about individual bacteria! In order to track the changes in abundances of individual prokaryotes throughout time, we need to first extract their genomes from the assembly. As mentioned above, their genomes will be highly fragmented, so the challenge is to “bin” these fragments (contigs or scaffolds) into groups. Ideally, each bin will be a single genome. Our grouping needs to be inclusive enough to get all the parts of each genome, but also specific enough to not include the wrong pieces. Sounds hard, right? It is! Luckily, there is software that can help us…\n",
    "\n",
    "One software tool that does this is metaBAT. Go ahead and conda install metabat2, and look over the usage examples and guidelines here. Note that you need to align the reads from multiple samples to the assembly to get the coverage of each contig in all the samples. We recommend that you use BWA to align reads to the assembly. Note that this should result in 8 separate alignment files that you will provide to metaBAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What metrics in the contigs can we use to group them together?\n",
    "<s>I think it is using kmers </s>\n",
    "Actually I think it is looking at overlaps between contigs, and then ranking their probability of being the same organism using the length of the overlap and the 'perfectness' of the matching overlap. \n",
    "### Question 3:\n",
    "##### (A) How many bins did you get?\n",
    "7 bins\n",
    "##### (B) Roughly what percentage of the assembly do they represent?\n",
    "grep \">\" 83_1.bam.1.fa | wc\n",
    "grep \">\" 83_1.bam.2.fa | wc\n",
    "grep \">\" 83_1.bam.3.fa | wc\n",
    "grep \">\" 83_1.bam.4.fa | wc\n",
    "grep \">\" 83_1.bam.5.fa | wc\n",
    "grep \">\" 83_1.bam.6.fa | wc\n",
    "grep \">\" 83_1.bam.7.fa | wc\n",
    "grep \">\" assembly.fasta | wc\n",
    "about 61% of the total assembly is contained within by bins\n",
    "##### (C) Do you think the sizes of each bin look about right, based on what you know about the size of prokaryotic genomes?\n",
    "wc 83_1.bam.1.fa\n",
    "wc 83_1.bam.2.fa\n",
    "wc 83_1.bam.3.fa\n",
    "wc 83_1.bam.4.fa\n",
    "wc 83_1.bam.5.fa\n",
    "wc 83_1.bam.6.fa\n",
    "wc 83_1.bam.7.fa\n",
    "Since they are all around a million base pairs (rough estimate), I think they are about right!\n",
    "##### (D) How would you estimate how complete and how contaminated each bin is?\n",
    "I would probably consult someone with more experience on this one, but without that I would compare the number of bins to the number of species in my Krona chart. I might also pick one of the bins and BLAST it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bwa index -a bwtsw assembly.fasta\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#for SAMPLE in 83_1 83_2 86_1 86_2 88_1 88_2 89_1 89_2 90_1 90_2 93_1 93_2 94_1 94_2 97_1 97_2\n",
    "#do\n",
    "#    bwa mem -R \"@RG\\tID:Sample1\\tSM:Sample1\" -o ${SAMPLE}.sam assembly.fasta SRR4921${SAMPLE}.fastq\n",
    "#done\n",
    "\n",
    "#chmod u+x bwa_mem.sh\n",
    "#./bwa_mem.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#for SAMPLE in 83_1 83_2 86_1 86_2 88_1 88_2 89_1 89_2 90_1 90_2 93_1 93_2 94_1 94_2 97_1 97_2\n",
    "#do\n",
    "#    samtools sort -o ${SAMPLE}.bam -O bam ${SAMPLE}.sam\n",
    "#done\n",
    "\n",
    "#chmod u+x sort_bam.sh\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#for SAMPLE in 83_1 83_2 86_1 86_2 88_1 88_2 89_1 89_2 90_1 90_2 93_1 93_2 94_1 94_2 97_1 97_2\n",
    "#do\n",
    "#    metabat2 -i assembly.fasta -o ${SAMPLE}.bam\n",
    "#done\n",
    "\n",
    "#chmod u+x metabat2.sh\n",
    "#./metabat2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Estimate the taxonomy of your putative genomes\n",
    "Rather than using your binning results, please download the desired output of binning here. Use this tar archive for Step 3.\n",
    "\n",
    "Now that you have individual genomes (we hope so, anyway…) we would like to know what they are. Luckily, you already have KRAKEN classifications of all the scaffolds (assembly.kraken), so you could use that as a reference. Cross reference the scaffolds in each bin and their respective KRAKEN taxonomies, and come up with your best prediction for what each bin represents.\n",
    "\n",
    "For example …\n",
    "$ head bin.1.fa\n",
    "\n",
    ">NODE_14_length_235766_cov_39.967778\n",
    "TGAATCACTCTATCTGCTTCTGTTTTTGCTGCTTCAAGTTCATCATGAATTTTAGTCATT\n",
    "TCATTATTGTATGCAGTCACGCTCGCTGGTTTCTTACCTTCGGTAACTCCCACATGATTT\n",
    "AATCCTGGACGTTTTTGTTCTAACACTGACTTATCTGCTTTTAATGCATTTTTTGCTTCA\n",
    "GTTAATTGAGTTAATGCTTCTTCTACTTTCGCTTTCTCATTTCTGATTTCTTCAGCTGTC\n",
    "GCATCTCCATTGTTAATCACGCCTTGGGCTTCTGCACTAATTCGCTCTGCTTCTACTTTT\n",
    "TTCGCTCTATAATTATCTGATGTTACTTGTGTCATTCCTGGCGTTGGATCTTGTTCTGCC\n",
    "GTTGCTTCATCAAGTTGACGTTTTGCTTCAACTAAAGCACTATTATCTGCTTTATTTTGT\n",
    "AGTAATGAAATTGCATGTTGAATTTTCAATGAGACATCATTAATATTATTTGTCACATTT\n",
    "GCAACATCAGTATTTGTTGCTCTATCATTTGATAACACTTCATTAGCTTTTCTTCGAACT\n",
    "\n",
    "$ grep NODE_14_length_235766_cov_39.967778 week13_data/KRAKEN/assembly.kraken\n",
    "\n",
    "NODE_14_length_235766_cov_39.967778\troot;cellular organisms;Bacteria;Terrabacteria group;Firmicutes;Bacilli;Bacillales;Staphylococcaceae;Staphylococcus;Staphylococcus haemolyticus;Staphylococcus haemolyticus JCSC1435\n",
    "\n",
    "But do this programatically and do something like tally up the various inferred species, family, order, etc.\n",
    "\n",
    "### Question 4:\n",
    "##### (A) What are your predictions for each bin?\n",
    "##### (B) This approach to classification is fast, but not very quantitative. Propose one method to more robustly infer the taxonomy of a metagenomic bin.\n",
    "if we had infinite time and computation then we could blast search each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode:\n",
    "    for file in bin file folder:\n",
    "        some way of going through the bin file and pulling out only the Node line\n",
    "        store the Node line as search_line\n",
    "        either make a list or write a file with all the search_lines in the bin file\n",
    "        \n",
    "        make dictionaries for each level of phylum\n",
    "        \n",
    "        for line in search_line_grouping:\n",
    "            open the assebly file\n",
    "            if file.startswith('line'):\n",
    "                parse the file\n",
    "                feild[4]=kingdom\n",
    "                add kingdom to the dictionary, if it is already there then add one to the value\n",
    "                do this for the different levels of phylogeny\n",
    "                \n",
    "                \n",
    "metabat2_script.shreference = open(assembly file, 'r')\n",
    "lines of the reference = reference.readlines()\n",
    "reference.close()\n",
    "node = []\n",
    "for thing in range (1,2):\n",
    "    binfile = open('bin file' + str(x) + '.fa', 'r')\n",
    "    #read all lines in\n",
    "    bins = binfile.readlines()\n",
    "    binfile.close()\n",
    "    \n",
    "    for stuff in bins:\n",
    "        if '>' in stuff:\n",
    "            node.append(z.strip().split('>')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/qbb2020-answers/week10/files/bins/bin.2.fa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a3b801fc6d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbin_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbin_number\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbin_numbers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbinfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/qbb2020-answers/week10/files/bins/bin.'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_number\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.fa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbinfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/qbb2020-answers/week10/files/bins/bin.2.fa'"
     ]
    }
   ],
   "source": [
    "#in command line\n",
    "# assembly.kraken awk '{gsub(\";\",\"\\t\"); print}' > assembly2.kraken\n",
    "\n",
    "bin_numbers = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "bin_info = []\n",
    "bin_contains = []\n",
    "for bin_number in bin_numbers:\n",
    "    binfile = open('~/qbb2020-answers/week10/files/bins/bin.'+ str(bin_number) + '.fa', 'r')\n",
    "    for line in binfile:\n",
    "        if line.startswith('>'):\n",
    "            bin_search_line = line[1:]\n",
    "            assemblyfile = open(~/qbb2020-answers/week10/files/week13_data/KRAKEN/assembly2.kraken, 'r')\n",
    "            for line in assemblyfile:\n",
    "                if line.startswith(bin_search_line):\n",
    "                    bin_contains.append([bin_number, line])\n",
    "                else:\n",
    "                    continue\n",
    "            assemblyfile.close()\n",
    "        else:\n",
    "            continue\n",
    "    binfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Make a heatmap of the individual bin abundances over time\n",
    "You have your genomes - now we want to see how their abundances shifted in the baby’s gut during these 8 days. Here are the abundances of each bin in each sample. This table contains the average bin coverage in each sample.\n",
    "\n",
    "Use your favorite software package to make a heatmap. All the samples have the same total read count, so you do not need to standardize your abundance values. Label the bins with their corresponding species/genus name (which you determined in Step 3). If you are unsure how to proceed, consider using the seaborn clustermap function.\n",
    "\n",
    "##### Question 5: Compare the results of the individual genome abundance analysis to the conclusions you derived from looking at the read taxonomy distributions (from Step 1). Do they agree with each other? What is different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk '{gsub(\";\",\"\\t\"); print}' assembly.kraken > assembly2.kraken\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
